{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras, os, pickle\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "\n",
    "from sklearn.model_selection import cross_validate, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from time import time\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import regularizers\n",
    "from keras.wrappers.scikit_learn import KerasRegressor # this is for making a model like every other in scikit\n",
    "import  matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "from tensorflow.random import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 110 targets and 23167 compounds currently loaded with 56392 interactions.\n",
      "A DTI matrix would be 2.213% dense!\n",
      "23167 fingerprints were loaded!\n",
      "Stats for values : -4.604582905766776 | 2.5887050795505413\n"
     ]
    }
   ],
   "source": [
    "Interactions_train = []    \n",
    "with open(\"Interactions_Trainset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_train.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions_valid = []        \n",
    "with open(\"Interactions_Validset.tab\",'r') as f:\n",
    "    for line in f:\n",
    "        tokens = line.split()\n",
    "        # 'Target-ID', 'Compound-ID', 'pIC50'  \n",
    "        Interactions_valid.append( [tokens[0], tokens[1], float(tokens[2]) ])\n",
    "\n",
    "Interactions = [x for x in Interactions_train]\n",
    "Interactions.extend(Interactions_valid)\n",
    "# we use a dataframe to quickly sort targets wrt #compounds:\n",
    "DF = pd.DataFrame( Interactions, columns =['Target-ID', 'Compound-ID','Std-value']) \n",
    "temp = DF.groupby(['Target-ID']).agg('count').sort_values(by='Compound-ID') # count the number of molecules\n",
    "Targets = list(temp.index)\n",
    "Compounds = np.unique(DF['Compound-ID'])\n",
    "del temp, DF\n",
    "\n",
    "nT=len(Targets); nC=len(Compounds)\n",
    "\n",
    "print(\"There are {0} targets and {1} compounds currently loaded with {2} interactions.\".format(nT,nC,len(Interactions)))\n",
    "print(\"A DTI matrix would be {0:.4}% dense!\".format(100.0*len(Interactions)/nT/nC ))\n",
    "\n",
    "# first we need to prepare each fp as a feature vector\n",
    "Fingerprints={} # this contains one list per fingerprint - not efficient...\n",
    "with open('Compound_Fingerprints.tab', 'r') as f:\n",
    "    header = f.readline()\n",
    "    for line in f:\n",
    "        # each line is Comp-ID, SMILES, FP\n",
    "        tokens = line.split()\n",
    "        # we keep only those compounds which have FPs\n",
    "        if tokens[2] != 'NOFP':\n",
    "            fp = [int(c) for c in tokens[2] ]\n",
    "            Fingerprints[ tokens[0] ] = fp\n",
    "print(\"%d fingerprints were loaded!\" % len(Fingerprints))\n",
    "\n",
    "# data standardisation - no need after using pIC50 !\n",
    "values = [x[2] for x in Interactions]\n",
    "print(\"Stats for values : {0} | {1}\".format(np.mean(values), np.std(values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forests with Self-Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import sem\n",
    "from scipy.stats import t as tstat\n",
    "def Imputer(model, NewID, Fingerprints, threshold=0.2):\n",
    "    \n",
    "    X_new = []\n",
    "    for cid in NewID:\n",
    "        x_test = np.array( Fingerprints[cid] ).reshape(1,-1)\n",
    "        preds=[]\n",
    "        for DTR in model.estimators_:\n",
    "            preds.append(DTR.predict(x_test) )\n",
    "        std_err = sem(preds)\n",
    "        h = std_err * tstat.ppf((1 + 0.95) / 2, len(preds) - 1)\n",
    "        if 2*h<=threshold:\n",
    "            X_new.append( [ cid, np.mean(preds)] )\n",
    "    if len(X_new)>0:\n",
    "        print(\"{0} new values were imputed!\".format(len(X_new)))\n",
    "    else:\n",
    "        print(\"No confident values were found.\")\n",
    "    return X_new\n",
    "\n",
    "def Evaluate_RF( TARGET, MODEL, validationset, prnt=False ):\n",
    "    True_temp = []; Pred_temp = []\n",
    "    with open( validationset, 'r') as file:\n",
    "        # no header on this file\n",
    "        for line in file:\n",
    "            tokens = line.split()\n",
    "            if tokens[0]==TARGET:\n",
    "                True_temp.append( float(tokens[2]) )\n",
    "                x_test = np.array( Fingerprints[tokens[1]] ).reshape(1,-1)\n",
    "                Pred_temp.append( MODEL.predict( x_test ) )\n",
    "    r2 = r2_score(True_temp,Pred_temp)\n",
    "    if prnt:\n",
    "        print(\"R2-score after {0} points = {1:.4f} \".format(len(True_temp), r2 ) )\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation without imputation = 0.8617 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.9250 \n",
      "More than 0 targets are processed\n",
      "Mean score so far: 0.924978\n",
      "Evaluation without imputation = 0.6512 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.8970 \n",
      "Evaluation without imputation = 0.8648 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.8834 \n",
      "Evaluation without imputation = 0.8377 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.8971 \n",
      "Evaluation without imputation = 0.6192 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.7425 \n",
      "Evaluation without imputation = 0.9859 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.9984 \n",
      "Evaluation without imputation = 0.8527 \n",
      "Imputing confident values...\n",
      "No confident values were found.\n",
      "Re-evaluate after imputation: 0.9108 \n",
      "Evaluation without imputation = 0.7973 \n",
      "Imputing confident values...\n"
     ]
    }
   ],
   "source": [
    "Target_info = {} # this is a \"global\" variable\n",
    "theta=0.1\n",
    "count=0\n",
    "for target in Targets:\n",
    "    Target_info[target] = {}\n",
    "\n",
    "    # define the train set\n",
    "    X_train=[]; Y_train=[]\n",
    "    Train_CIDs = []\n",
    "    for point in Interactions:\n",
    "        if point[0]==target:\n",
    "            X_train.append( Fingerprints[point[1]] )\n",
    "            Y_train.append( float(point[2]) )\n",
    "            Train_CIDs.append( point[1] )\n",
    "    Target_info[target]['train_size']=len(Y_train) # add info\n",
    "    \n",
    "    with open( 'D:/Sem8_FYP/TrainedModals/RF_'+target+'_'+'pIC50new.sav', 'rb') as f:\n",
    "            MODEL = pickle.load( f )\n",
    "            \n",
    "    Target_info[target]['first_r2'] = MODEL.score( X_train,  Y_train) # add info\n",
    "    print(\"Evaluation without imputation = %.4f \" % Target_info[target][\"first_r2\"] )    \n",
    "    \n",
    "    print(\"Imputing confident values...\")\n",
    "    X_new = [1] # just a trigger for the next loop\n",
    "    while  (len(X_new)>0) & (len(Train_CIDs)<2000):\n",
    "        # we need to stop after we have no new predictions or we have enough (10K+)\n",
    "        # update the train set\n",
    "        NewIDs = [x for x in Compounds if x not in Train_CIDs] # terra incognito\n",
    "        X_new = Imputer(MODEL, NewIDs, Fingerprints, threshold=theta)\n",
    "        \n",
    "        for point in X_new:\n",
    "            X_train.append( Fingerprints[point[0]] )\n",
    "            Y_train.append( float(point[1]) )\n",
    "            Train_CIDs.append( point[0] )\n",
    "        # re-train\n",
    "        MODEL.fit(np.array( X_train ),Y_train)\n",
    "\n",
    "    Target_info[target][\"model\"] = MODEL\n",
    "    # evaluate again as before \n",
    "    True_temp = []\n",
    "    Pred_temp = []\n",
    "    for point in Interactions_valid:\n",
    "        if point[0]==target:\n",
    "            True_temp.append( float(point[2]) )\n",
    "            x_test = np.array( Fingerprints[point[1]] ).reshape(1,-1)\n",
    "            Pred_temp.append( MODEL.predict( x_test ) )\n",
    "    Target_info[target][\"after_r2\"] = r2_score(True_temp,Pred_temp)\n",
    "    print(\"Re-evaluate after imputation: %.4f \" % Target_info[target][\"after_r2\"] )\n",
    "    \n",
    "    if count%25==0:\n",
    "        print(\"More than %d targets are processed\" % count)\n",
    "        temp = np.mean( [Target_info[t][\"after_r2\"] for t in Target_info.keys()] )\n",
    "        print(\"Mean score so far: %f\" %  temp)\n",
    "    count+=1\n",
    "print(\"Overall accuracy for self-trained RF = \",  np.mean( [Target_info[t][\"after_r2\"] for t in Targets] ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
